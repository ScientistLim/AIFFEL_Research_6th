{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRZqkNqvJgHq"
   },
   "source": [
    "# 프로젝트 1 : 손수 설계하는 선형회귀, 당뇨병 수치를 맞춰보자!\n",
    "\n",
    "직접 손실함수와 기울기를 계산하지 않고, 사이킷런(sklearn)의 LinearRegression 모델을 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO7aQ5nCKagT"
   },
   "source": [
    "## 데이터 가져오기\n",
    "\n",
    "- sklearn.datasets의 load_diabetes에서 데이터 로드\n",
    "- diabetes의 data를 df_X에, target을 df_y에 *저장*\n",
    "\n",
    "## 모델에 입력할 데이터 X,y 준비하기\n",
    "df_X, df_y에 있는 값들을 numpy array로 변환해서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ya1FJeHJfbB",
    "outputId": "3005ca99-8ca2-4f1b-ad54-bbbbdc1c3b30"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes()\n",
    "df_X, df_y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3FkndLDW2e8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVl-TuMJJv3q"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NQB4sANKD8z"
   },
   "source": [
    "## train 데이터와 test 데이터로 분리하기\n",
    "X와 y 데이터를 각각 train 데이터와 test 데이터로 분리해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uicBOT8WK_tj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bx_pxlCQS7Fj",
    "outputId": "61ce6c0b-460b-48a4-9ef8-4a3986cec99c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYJEDX00KFBB"
   },
   "source": [
    "## 모델 준비하기\n",
    "입력 데이터 개수에 맞는 가중치 W와 b를 준비해주세요.\n",
    "모델 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "70qwx4IMLAYQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "W = np.random.rand(X_train.shape[1])\n",
    "b = np.random.rand()\n",
    "\n",
    "def model(X, W, b):\n",
    "    return X.dot(W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xME28ghfKGeD"
   },
   "source": [
    "## 손실함수 loss 정의하기\n",
    "손실함수를 MSE 함수로 정의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xiD2UI1-LA52"
   },
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQuur0_cKLwq"
   },
   "source": [
    "## 기울기를 구하는 gradient 함수 구현하기\n",
    "기울기를 계산하는 gradient 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cUE4XMgLBe6",
    "outputId": "14ba2100-c3a1-4db7-c396-8cef8f963a77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.02563948,  0.17532074, -4.44473977, -2.27196164, -1.48762285,\n",
       "        -1.53469847,  3.18860775, -3.35905302, -3.90381758, -2.24080565]),\n",
       " -303.26761168090763)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient(X, y, y_pred):\n",
    "    N = len(y)\n",
    "    dW = 2/N * X.T.dot(y_pred - y)\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db\n",
    "\n",
    "y_pred = model(X_train, W, b)\n",
    "dW, db = gradient(X_train, y_train, y_pred)\n",
    "dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2Ch8NJhKNI3"
   },
   "source": [
    "## 하이퍼 파라미터인 학습률 설정하기\n",
    "학습률, learning rate 를 설정해주세요\n",
    "만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1OG7KGRVLCOI"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfKojLqgKPYK"
   },
   "source": [
    "## 모델 학습하기\n",
    "정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "입력하는 데이터인 X에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8e6p292LC2w",
    "outputId": "3fb8e214-29ac-4402-f374-04568048794e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : Loss 28833.1948\n",
      "Iteration 10 : Loss 5515.1816\n",
      "Iteration 20 : Loss 5236.9257\n",
      "Iteration 30 : Loss 4997.9239\n",
      "Iteration 40 : Loss 4792.0025\n",
      "Iteration 50 : Loss 4613.9925\n",
      "Iteration 60 : Loss 4459.5644\n",
      "Iteration 70 : Loss 4325.0904\n",
      "Iteration 80 : Loss 4207.5289\n",
      "Iteration 90 : Loss 4104.3287\n",
      "Iteration 100 : Loss 4013.3484\n",
      "Iteration 110 : Loss 3932.7890\n",
      "Iteration 120 : Loss 3861.1387\n",
      "Iteration 130 : Loss 3797.1252\n",
      "Iteration 140 : Loss 3739.6768\n",
      "Iteration 150 : Loss 3687.8902\n",
      "Iteration 160 : Loss 3641.0026\n",
      "Iteration 170 : Loss 3598.3690\n",
      "Iteration 180 : Loss 3559.4433\n",
      "Iteration 190 : Loss 3523.7625\n",
      "Iteration 200 : Loss 3490.9328\n",
      "Iteration 210 : Loss 3460.6189\n",
      "Iteration 220 : Loss 3432.5347\n",
      "Iteration 230 : Loss 3406.4352\n",
      "Iteration 240 : Loss 3382.1103\n",
      "Iteration 250 : Loss 3359.3791\n",
      "Iteration 260 : Loss 3338.0853\n",
      "Iteration 270 : Loss 3318.0937\n",
      "Iteration 280 : Loss 3299.2866\n",
      "Iteration 290 : Loss 3281.5613\n",
      "Iteration 300 : Loss 3264.8279\n",
      "Iteration 310 : Loss 3249.0072\n",
      "Iteration 320 : Loss 3234.0291\n",
      "Iteration 330 : Loss 3219.8318\n",
      "Iteration 340 : Loss 3206.3598\n",
      "Iteration 350 : Loss 3193.5636\n",
      "Iteration 360 : Loss 3181.3988\n",
      "Iteration 370 : Loss 3169.8251\n",
      "Iteration 380 : Loss 3158.8061\n",
      "Iteration 390 : Loss 3148.3086\n",
      "Iteration 400 : Loss 3138.3024\n",
      "Iteration 410 : Loss 3128.7595\n",
      "Iteration 420 : Loss 3119.6544\n",
      "Iteration 430 : Loss 3110.9635\n",
      "Iteration 440 : Loss 3102.6648\n",
      "Iteration 450 : Loss 3094.7380\n",
      "Iteration 460 : Loss 3087.1641\n",
      "Iteration 470 : Loss 3079.9254\n",
      "Iteration 480 : Loss 3073.0053\n",
      "Iteration 490 : Loss 3066.3882\n",
      "Iteration 500 : Loss 3060.0596\n",
      "Iteration 510 : Loss 3054.0056\n",
      "Iteration 520 : Loss 3048.2133\n",
      "Iteration 530 : Loss 3042.6704\n",
      "Iteration 540 : Loss 3037.3654\n",
      "Iteration 550 : Loss 3032.2872\n",
      "Iteration 560 : Loss 3027.4254\n",
      "Iteration 570 : Loss 3022.7703\n",
      "Iteration 580 : Loss 3018.3124\n",
      "Iteration 590 : Loss 3014.0429\n",
      "Iteration 600 : Loss 3009.9534\n",
      "Iteration 610 : Loss 3006.0357\n",
      "Iteration 620 : Loss 3002.2823\n",
      "Iteration 630 : Loss 2998.6858\n",
      "Iteration 640 : Loss 2995.2394\n",
      "Iteration 650 : Loss 2991.9365\n",
      "Iteration 660 : Loss 2988.7707\n",
      "Iteration 670 : Loss 2985.7361\n",
      "Iteration 680 : Loss 2982.8270\n",
      "Iteration 690 : Loss 2980.0378\n",
      "Iteration 700 : Loss 2977.3634\n",
      "Iteration 710 : Loss 2974.7989\n",
      "Iteration 720 : Loss 2972.3394\n",
      "Iteration 730 : Loss 2969.9805\n",
      "Iteration 740 : Loss 2967.7178\n",
      "Iteration 750 : Loss 2965.5472\n",
      "Iteration 760 : Loss 2963.4648\n",
      "Iteration 770 : Loss 2961.4668\n",
      "Iteration 780 : Loss 2959.5496\n",
      "Iteration 790 : Loss 2957.7098\n",
      "Iteration 800 : Loss 2955.9440\n",
      "Iteration 810 : Loss 2954.2491\n",
      "Iteration 820 : Loss 2952.6222\n",
      "Iteration 830 : Loss 2951.0604\n",
      "Iteration 840 : Loss 2949.5609\n",
      "Iteration 850 : Loss 2948.1211\n",
      "Iteration 860 : Loss 2946.7385\n",
      "Iteration 870 : Loss 2945.4106\n",
      "Iteration 880 : Loss 2944.1353\n",
      "Iteration 890 : Loss 2942.9103\n",
      "Iteration 900 : Loss 2941.7335\n",
      "Iteration 910 : Loss 2940.6029\n",
      "Iteration 920 : Loss 2939.5166\n",
      "Iteration 930 : Loss 2938.4727\n",
      "Iteration 940 : Loss 2937.4695\n",
      "Iteration 950 : Loss 2936.5053\n",
      "Iteration 960 : Loss 2935.5785\n",
      "Iteration 970 : Loss 2934.6876\n",
      "Iteration 980 : Loss 2933.8310\n",
      "Iteration 990 : Loss 2933.0074\n",
      "Iteration 1000 : Loss 2932.2155\n",
      "Iteration 1010 : Loss 2931.4538\n",
      "Iteration 1020 : Loss 2930.7213\n",
      "Iteration 1030 : Loss 2930.0166\n",
      "Iteration 1040 : Loss 2929.3387\n",
      "Iteration 1050 : Loss 2928.6865\n",
      "Iteration 1060 : Loss 2928.0589\n",
      "Iteration 1070 : Loss 2927.4550\n",
      "Iteration 1080 : Loss 2926.8737\n",
      "Iteration 1090 : Loss 2926.3142\n",
      "Iteration 1100 : Loss 2925.7756\n",
      "Iteration 1110 : Loss 2925.2571\n",
      "Iteration 1120 : Loss 2924.7578\n",
      "Iteration 1130 : Loss 2924.2770\n",
      "Iteration 1140 : Loss 2923.8140\n",
      "Iteration 1150 : Loss 2923.3679\n",
      "Iteration 1160 : Loss 2922.9383\n",
      "Iteration 1170 : Loss 2922.5243\n",
      "Iteration 1180 : Loss 2922.1254\n",
      "Iteration 1190 : Loss 2921.7411\n",
      "Iteration 1200 : Loss 2921.3706\n",
      "Iteration 1210 : Loss 2921.0135\n",
      "Iteration 1220 : Loss 2920.6692\n",
      "Iteration 1230 : Loss 2920.3373\n",
      "Iteration 1240 : Loss 2920.0172\n",
      "Iteration 1250 : Loss 2919.7086\n",
      "Iteration 1260 : Loss 2919.4108\n",
      "Iteration 1270 : Loss 2919.1236\n",
      "Iteration 1280 : Loss 2918.8466\n",
      "Iteration 1290 : Loss 2918.5792\n",
      "Iteration 1300 : Loss 2918.3212\n",
      "Iteration 1310 : Loss 2918.0721\n",
      "Iteration 1320 : Loss 2917.8317\n",
      "Iteration 1330 : Loss 2917.5996\n",
      "Iteration 1340 : Loss 2917.3755\n",
      "Iteration 1350 : Loss 2917.1591\n",
      "Iteration 1360 : Loss 2916.9501\n",
      "Iteration 1370 : Loss 2916.7482\n",
      "Iteration 1380 : Loss 2916.5531\n",
      "Iteration 1390 : Loss 2916.3646\n",
      "Iteration 1400 : Loss 2916.1824\n",
      "Iteration 1410 : Loss 2916.0063\n",
      "Iteration 1420 : Loss 2915.8361\n",
      "Iteration 1430 : Loss 2915.6715\n",
      "Iteration 1440 : Loss 2915.5123\n",
      "Iteration 1450 : Loss 2915.3584\n",
      "Iteration 1460 : Loss 2915.2095\n",
      "Iteration 1470 : Loss 2915.0654\n",
      "Iteration 1480 : Loss 2914.9260\n",
      "Iteration 1490 : Loss 2914.7911\n",
      "Iteration 1500 : Loss 2914.6606\n",
      "Iteration 1510 : Loss 2914.5341\n",
      "Iteration 1520 : Loss 2914.4118\n",
      "Iteration 1530 : Loss 2914.2932\n",
      "Iteration 1540 : Loss 2914.1784\n",
      "Iteration 1550 : Loss 2914.0672\n",
      "Iteration 1560 : Loss 2913.9594\n",
      "Iteration 1570 : Loss 2913.8550\n",
      "Iteration 1580 : Loss 2913.7538\n",
      "Iteration 1590 : Loss 2913.6557\n",
      "Iteration 1600 : Loss 2913.5606\n",
      "Iteration 1610 : Loss 2913.4683\n",
      "Iteration 1620 : Loss 2913.3788\n",
      "Iteration 1630 : Loss 2913.2920\n",
      "Iteration 1640 : Loss 2913.2078\n",
      "Iteration 1650 : Loss 2913.1261\n",
      "Iteration 1660 : Loss 2913.0467\n",
      "Iteration 1670 : Loss 2912.9697\n",
      "Iteration 1680 : Loss 2912.8949\n",
      "Iteration 1690 : Loss 2912.8223\n",
      "Iteration 1700 : Loss 2912.7517\n",
      "Iteration 1710 : Loss 2912.6832\n",
      "Iteration 1720 : Loss 2912.6166\n",
      "Iteration 1730 : Loss 2912.5518\n",
      "Iteration 1740 : Loss 2912.4889\n",
      "Iteration 1750 : Loss 2912.4277\n",
      "Iteration 1760 : Loss 2912.3682\n",
      "Iteration 1770 : Loss 2912.3103\n",
      "Iteration 1780 : Loss 2912.2540\n",
      "Iteration 1790 : Loss 2912.1992\n",
      "Iteration 1800 : Loss 2912.1459\n",
      "Iteration 1810 : Loss 2912.0939\n",
      "Iteration 1820 : Loss 2912.0434\n",
      "Iteration 1830 : Loss 2911.9942\n",
      "Iteration 1840 : Loss 2911.9462\n",
      "Iteration 1850 : Loss 2911.8995\n",
      "Iteration 1860 : Loss 2911.8539\n",
      "Iteration 1870 : Loss 2911.8096\n",
      "Iteration 1880 : Loss 2911.7663\n",
      "Iteration 1890 : Loss 2911.7241\n",
      "Iteration 1900 : Loss 2911.6830\n",
      "Iteration 1910 : Loss 2911.6428\n",
      "Iteration 1920 : Loss 2911.6036\n",
      "Iteration 1930 : Loss 2911.5654\n",
      "Iteration 1940 : Loss 2911.5281\n",
      "Iteration 1950 : Loss 2911.4917\n",
      "Iteration 1960 : Loss 2911.4561\n",
      "Iteration 1970 : Loss 2911.4213\n",
      "Iteration 1980 : Loss 2911.3874\n",
      "Iteration 1990 : Loss 2911.3542\n",
      "Iteration 2000 : Loss 2911.3218\n",
      "Iteration 2010 : Loss 2911.2901\n",
      "Iteration 2020 : Loss 2911.2591\n",
      "Iteration 2030 : Loss 2911.2287\n",
      "Iteration 2040 : Loss 2911.1991\n",
      "Iteration 2050 : Loss 2911.1701\n",
      "Iteration 2060 : Loss 2911.1416\n",
      "Iteration 2070 : Loss 2911.1138\n",
      "Iteration 2080 : Loss 2911.0866\n",
      "Iteration 2090 : Loss 2911.0599\n",
      "Iteration 2100 : Loss 2911.0338\n",
      "Iteration 2110 : Loss 2911.0082\n",
      "Iteration 2120 : Loss 2910.9831\n",
      "Iteration 2130 : Loss 2910.9585\n",
      "Iteration 2140 : Loss 2910.9344\n",
      "Iteration 2150 : Loss 2910.9107\n",
      "Iteration 2160 : Loss 2910.8875\n",
      "Iteration 2170 : Loss 2910.8648\n",
      "Iteration 2180 : Loss 2910.8424\n",
      "Iteration 2190 : Loss 2910.8205\n",
      "Iteration 2200 : Loss 2910.7990\n",
      "Iteration 2210 : Loss 2910.7779\n",
      "Iteration 2220 : Loss 2910.7571\n",
      "Iteration 2230 : Loss 2910.7367\n",
      "Iteration 2240 : Loss 2910.7167\n",
      "Iteration 2250 : Loss 2910.6970\n",
      "Iteration 2260 : Loss 2910.6776\n",
      "Iteration 2270 : Loss 2910.6585\n",
      "Iteration 2280 : Loss 2910.6398\n",
      "Iteration 2290 : Loss 2910.6214\n",
      "Iteration 2300 : Loss 2910.6033\n",
      "Iteration 2310 : Loss 2910.5854\n",
      "Iteration 2320 : Loss 2910.5679\n",
      "Iteration 2330 : Loss 2910.5506\n",
      "Iteration 2340 : Loss 2910.5336\n",
      "Iteration 2350 : Loss 2910.5168\n",
      "Iteration 2360 : Loss 2910.5003\n",
      "Iteration 2370 : Loss 2910.4840\n",
      "Iteration 2380 : Loss 2910.4679\n",
      "Iteration 2390 : Loss 2910.4521\n",
      "Iteration 2400 : Loss 2910.4365\n",
      "Iteration 2410 : Loss 2910.4211\n",
      "Iteration 2420 : Loss 2910.4060\n",
      "Iteration 2430 : Loss 2910.3910\n",
      "Iteration 2440 : Loss 2910.3762\n",
      "Iteration 2450 : Loss 2910.3617\n",
      "Iteration 2460 : Loss 2910.3473\n",
      "Iteration 2470 : Loss 2910.3331\n",
      "Iteration 2480 : Loss 2910.3190\n",
      "Iteration 2490 : Loss 2910.3052\n",
      "Iteration 2500 : Loss 2910.2915\n",
      "Iteration 2510 : Loss 2910.2780\n",
      "Iteration 2520 : Loss 2910.2646\n",
      "Iteration 2530 : Loss 2910.2514\n",
      "Iteration 2540 : Loss 2910.2383\n",
      "Iteration 2550 : Loss 2910.2254\n",
      "Iteration 2560 : Loss 2910.2126\n",
      "Iteration 2570 : Loss 2910.2000\n",
      "Iteration 2580 : Loss 2910.1875\n",
      "Iteration 2590 : Loss 2910.1751\n",
      "Iteration 2600 : Loss 2910.1629\n",
      "Iteration 2610 : Loss 2910.1507\n",
      "Iteration 2620 : Loss 2910.1387\n",
      "Iteration 2630 : Loss 2910.1269\n",
      "Iteration 2640 : Loss 2910.1151\n",
      "Iteration 2650 : Loss 2910.1034\n",
      "Iteration 2660 : Loss 2910.0919\n",
      "Iteration 2670 : Loss 2910.0804\n",
      "Iteration 2680 : Loss 2910.0691\n",
      "Iteration 2690 : Loss 2910.0579\n",
      "Iteration 2700 : Loss 2910.0467\n",
      "Iteration 2710 : Loss 2910.0357\n",
      "Iteration 2720 : Loss 2910.0248\n",
      "Iteration 2730 : Loss 2910.0139\n",
      "Iteration 2740 : Loss 2910.0031\n",
      "Iteration 2750 : Loss 2909.9924\n",
      "Iteration 2760 : Loss 2909.9818\n",
      "Iteration 2770 : Loss 2909.9713\n",
      "Iteration 2780 : Loss 2909.9609\n",
      "Iteration 2790 : Loss 2909.9505\n",
      "Iteration 2800 : Loss 2909.9403\n",
      "Iteration 2810 : Loss 2909.9300\n",
      "Iteration 2820 : Loss 2909.9199\n",
      "Iteration 2830 : Loss 2909.9098\n",
      "Iteration 2840 : Loss 2909.8999\n",
      "Iteration 2850 : Loss 2909.8899\n",
      "Iteration 2860 : Loss 2909.8801\n",
      "Iteration 2870 : Loss 2909.8703\n",
      "Iteration 2880 : Loss 2909.8605\n",
      "Iteration 2890 : Loss 2909.8509\n",
      "Iteration 2900 : Loss 2909.8412\n",
      "Iteration 2910 : Loss 2909.8317\n",
      "Iteration 2920 : Loss 2909.8222\n",
      "Iteration 2930 : Loss 2909.8127\n",
      "Iteration 2940 : Loss 2909.8033\n",
      "Iteration 2950 : Loss 2909.7940\n",
      "Iteration 2960 : Loss 2909.7847\n",
      "Iteration 2970 : Loss 2909.7755\n",
      "Iteration 2980 : Loss 2909.7663\n",
      "Iteration 2990 : Loss 2909.7571\n",
      "Iteration 3000 : Loss 2909.7480\n",
      "Iteration 3010 : Loss 2909.7390\n",
      "Iteration 3020 : Loss 2909.7300\n",
      "Iteration 3030 : Loss 2909.7210\n",
      "Iteration 3040 : Loss 2909.7121\n",
      "Iteration 3050 : Loss 2909.7032\n",
      "Iteration 3060 : Loss 2909.6944\n",
      "Iteration 3070 : Loss 2909.6856\n",
      "Iteration 3080 : Loss 2909.6768\n",
      "Iteration 3090 : Loss 2909.6681\n",
      "Iteration 3100 : Loss 2909.6594\n",
      "Iteration 3110 : Loss 2909.6508\n",
      "Iteration 3120 : Loss 2909.6422\n",
      "Iteration 3130 : Loss 2909.6336\n",
      "Iteration 3140 : Loss 2909.6251\n",
      "Iteration 3150 : Loss 2909.6166\n",
      "Iteration 3160 : Loss 2909.6081\n",
      "Iteration 3170 : Loss 2909.5996\n",
      "Iteration 3180 : Loss 2909.5912\n",
      "Iteration 3190 : Loss 2909.5828\n",
      "Iteration 3200 : Loss 2909.5745\n",
      "Iteration 3210 : Loss 2909.5662\n",
      "Iteration 3220 : Loss 2909.5579\n",
      "Iteration 3230 : Loss 2909.5496\n",
      "Iteration 3240 : Loss 2909.5413\n",
      "Iteration 3250 : Loss 2909.5331\n",
      "Iteration 3260 : Loss 2909.5249\n",
      "Iteration 3270 : Loss 2909.5168\n",
      "Iteration 3280 : Loss 2909.5086\n",
      "Iteration 3290 : Loss 2909.5005\n",
      "Iteration 3300 : Loss 2909.4924\n",
      "Iteration 3310 : Loss 2909.4844\n",
      "Iteration 3320 : Loss 2909.4763\n",
      "Iteration 3330 : Loss 2909.4683\n",
      "Iteration 3340 : Loss 2909.4603\n",
      "Iteration 3350 : Loss 2909.4523\n",
      "Iteration 3360 : Loss 2909.4443\n",
      "Iteration 3370 : Loss 2909.4364\n",
      "Iteration 3380 : Loss 2909.4285\n",
      "Iteration 3390 : Loss 2909.4206\n",
      "Iteration 3400 : Loss 2909.4127\n",
      "Iteration 3410 : Loss 2909.4048\n",
      "Iteration 3420 : Loss 2909.3970\n",
      "Iteration 3430 : Loss 2909.3892\n",
      "Iteration 3440 : Loss 2909.3814\n",
      "Iteration 3450 : Loss 2909.3736\n",
      "Iteration 3460 : Loss 2909.3658\n",
      "Iteration 3470 : Loss 2909.3581\n",
      "Iteration 3480 : Loss 2909.3503\n",
      "Iteration 3490 : Loss 2909.3426\n",
      "Iteration 3500 : Loss 2909.3349\n",
      "Iteration 3510 : Loss 2909.3272\n",
      "Iteration 3520 : Loss 2909.3195\n",
      "Iteration 3530 : Loss 2909.3119\n",
      "Iteration 3540 : Loss 2909.3042\n",
      "Iteration 3550 : Loss 2909.2966\n",
      "Iteration 3560 : Loss 2909.2890\n",
      "Iteration 3570 : Loss 2909.2814\n",
      "Iteration 3580 : Loss 2909.2738\n",
      "Iteration 3590 : Loss 2909.2662\n",
      "Iteration 3600 : Loss 2909.2586\n",
      "Iteration 3610 : Loss 2909.2511\n",
      "Iteration 3620 : Loss 2909.2435\n",
      "Iteration 3630 : Loss 2909.2360\n",
      "Iteration 3640 : Loss 2909.2285\n",
      "Iteration 3650 : Loss 2909.2210\n",
      "Iteration 3660 : Loss 2909.2135\n",
      "Iteration 3670 : Loss 2909.2060\n",
      "Iteration 3680 : Loss 2909.1986\n",
      "Iteration 3690 : Loss 2909.1911\n",
      "Iteration 3700 : Loss 2909.1837\n",
      "Iteration 3710 : Loss 2909.1762\n",
      "Iteration 3720 : Loss 2909.1688\n",
      "Iteration 3730 : Loss 2909.1614\n",
      "Iteration 3740 : Loss 2909.1540\n",
      "Iteration 3750 : Loss 2909.1466\n",
      "Iteration 3760 : Loss 2909.1392\n",
      "Iteration 3770 : Loss 2909.1318\n",
      "Iteration 3780 : Loss 2909.1245\n",
      "Iteration 3790 : Loss 2909.1171\n",
      "Iteration 3800 : Loss 2909.1098\n",
      "Iteration 3810 : Loss 2909.1024\n",
      "Iteration 3820 : Loss 2909.0951\n",
      "Iteration 3830 : Loss 2909.0878\n",
      "Iteration 3840 : Loss 2909.0805\n",
      "Iteration 3850 : Loss 2909.0732\n",
      "Iteration 3860 : Loss 2909.0659\n",
      "Iteration 3870 : Loss 2909.0586\n",
      "Iteration 3880 : Loss 2909.0513\n",
      "Iteration 3890 : Loss 2909.0440\n",
      "Iteration 3900 : Loss 2909.0368\n",
      "Iteration 3910 : Loss 2909.0295\n",
      "Iteration 3920 : Loss 2909.0223\n",
      "Iteration 3930 : Loss 2909.0150\n",
      "Iteration 3940 : Loss 2909.0078\n",
      "Iteration 3950 : Loss 2909.0006\n",
      "Iteration 3960 : Loss 2908.9933\n",
      "Iteration 3970 : Loss 2908.9861\n",
      "Iteration 3980 : Loss 2908.9789\n",
      "Iteration 3990 : Loss 2908.9717\n",
      "Iteration 4000 : Loss 2908.9645\n",
      "Iteration 4010 : Loss 2908.9574\n",
      "Iteration 4020 : Loss 2908.9502\n",
      "Iteration 4030 : Loss 2908.9430\n",
      "Iteration 4040 : Loss 2908.9358\n",
      "Iteration 4050 : Loss 2908.9287\n",
      "Iteration 4060 : Loss 2908.9215\n",
      "Iteration 4070 : Loss 2908.9144\n",
      "Iteration 4080 : Loss 2908.9072\n",
      "Iteration 4090 : Loss 2908.9001\n",
      "Iteration 4100 : Loss 2908.8930\n",
      "Iteration 4110 : Loss 2908.8859\n",
      "Iteration 4120 : Loss 2908.8787\n",
      "Iteration 4130 : Loss 2908.8716\n",
      "Iteration 4140 : Loss 2908.8645\n",
      "Iteration 4150 : Loss 2908.8574\n",
      "Iteration 4160 : Loss 2908.8503\n",
      "Iteration 4170 : Loss 2908.8432\n",
      "Iteration 4180 : Loss 2908.8361\n",
      "Iteration 4190 : Loss 2908.8291\n",
      "Iteration 4200 : Loss 2908.8220\n",
      "Iteration 4210 : Loss 2908.8149\n",
      "Iteration 4220 : Loss 2908.8079\n",
      "Iteration 4230 : Loss 2908.8008\n",
      "Iteration 4240 : Loss 2908.7937\n",
      "Iteration 4250 : Loss 2908.7867\n",
      "Iteration 4260 : Loss 2908.7797\n",
      "Iteration 4270 : Loss 2908.7726\n",
      "Iteration 4280 : Loss 2908.7656\n",
      "Iteration 4290 : Loss 2908.7585\n",
      "Iteration 4300 : Loss 2908.7515\n",
      "Iteration 4310 : Loss 2908.7445\n",
      "Iteration 4320 : Loss 2908.7375\n",
      "Iteration 4330 : Loss 2908.7305\n",
      "Iteration 4340 : Loss 2908.7235\n",
      "Iteration 4350 : Loss 2908.7165\n",
      "Iteration 4360 : Loss 2908.7095\n",
      "Iteration 4370 : Loss 2908.7025\n",
      "Iteration 4380 : Loss 2908.6955\n",
      "Iteration 4390 : Loss 2908.6885\n",
      "Iteration 4400 : Loss 2908.6815\n",
      "Iteration 4410 : Loss 2908.6745\n",
      "Iteration 4420 : Loss 2908.6676\n",
      "Iteration 4430 : Loss 2908.6606\n",
      "Iteration 4440 : Loss 2908.6536\n",
      "Iteration 4450 : Loss 2908.6467\n",
      "Iteration 4460 : Loss 2908.6397\n",
      "Iteration 4470 : Loss 2908.6327\n",
      "Iteration 4480 : Loss 2908.6258\n",
      "Iteration 4490 : Loss 2908.6188\n",
      "Iteration 4500 : Loss 2908.6119\n",
      "Iteration 4510 : Loss 2908.6050\n",
      "Iteration 4520 : Loss 2908.5980\n",
      "Iteration 4530 : Loss 2908.5911\n",
      "Iteration 4540 : Loss 2908.5842\n",
      "Iteration 4550 : Loss 2908.5772\n",
      "Iteration 4560 : Loss 2908.5703\n",
      "Iteration 4570 : Loss 2908.5634\n",
      "Iteration 4580 : Loss 2908.5565\n",
      "Iteration 4590 : Loss 2908.5496\n",
      "Iteration 4600 : Loss 2908.5427\n",
      "Iteration 4610 : Loss 2908.5358\n",
      "Iteration 4620 : Loss 2908.5289\n",
      "Iteration 4630 : Loss 2908.5220\n",
      "Iteration 4640 : Loss 2908.5151\n",
      "Iteration 4650 : Loss 2908.5082\n",
      "Iteration 4660 : Loss 2908.5013\n",
      "Iteration 4670 : Loss 2908.4944\n",
      "Iteration 4680 : Loss 2908.4876\n",
      "Iteration 4690 : Loss 2908.4807\n",
      "Iteration 4700 : Loss 2908.4738\n",
      "Iteration 4710 : Loss 2908.4669\n",
      "Iteration 4720 : Loss 2908.4601\n",
      "Iteration 4730 : Loss 2908.4532\n",
      "Iteration 4740 : Loss 2908.4463\n",
      "Iteration 4750 : Loss 2908.4395\n",
      "Iteration 4760 : Loss 2908.4326\n",
      "Iteration 4770 : Loss 2908.4258\n",
      "Iteration 4780 : Loss 2908.4189\n",
      "Iteration 4790 : Loss 2908.4121\n",
      "Iteration 4800 : Loss 2908.4053\n",
      "Iteration 4810 : Loss 2908.3984\n",
      "Iteration 4820 : Loss 2908.3916\n",
      "Iteration 4830 : Loss 2908.3848\n",
      "Iteration 4840 : Loss 2908.3779\n",
      "Iteration 4850 : Loss 2908.3711\n",
      "Iteration 4860 : Loss 2908.3643\n",
      "Iteration 4870 : Loss 2908.3575\n",
      "Iteration 4880 : Loss 2908.3506\n",
      "Iteration 4890 : Loss 2908.3438\n",
      "Iteration 4900 : Loss 2908.3370\n",
      "Iteration 4910 : Loss 2908.3302\n",
      "Iteration 4920 : Loss 2908.3234\n",
      "Iteration 4930 : Loss 2908.3166\n",
      "Iteration 4940 : Loss 2908.3098\n",
      "Iteration 4950 : Loss 2908.3030\n",
      "Iteration 4960 : Loss 2908.2962\n",
      "Iteration 4970 : Loss 2908.2894\n",
      "Iteration 4980 : Loss 2908.2826\n",
      "Iteration 4990 : Loss 2908.2758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000 : Loss 2908.2691\n",
      "Iteration 5010 : Loss 2908.2623\n",
      "Iteration 5020 : Loss 2908.2555\n",
      "Iteration 5030 : Loss 2908.2487\n",
      "Iteration 5040 : Loss 2908.2420\n",
      "Iteration 5050 : Loss 2908.2352\n",
      "Iteration 5060 : Loss 2908.2284\n",
      "Iteration 5070 : Loss 2908.2217\n",
      "Iteration 5080 : Loss 2908.2149\n",
      "Iteration 5090 : Loss 2908.2082\n",
      "Iteration 5100 : Loss 2908.2014\n",
      "Iteration 5110 : Loss 2908.1946\n",
      "Iteration 5120 : Loss 2908.1879\n",
      "Iteration 5130 : Loss 2908.1812\n",
      "Iteration 5140 : Loss 2908.1744\n",
      "Iteration 5150 : Loss 2908.1677\n",
      "Iteration 5160 : Loss 2908.1609\n",
      "Iteration 5170 : Loss 2908.1542\n",
      "Iteration 5180 : Loss 2908.1475\n",
      "Iteration 5190 : Loss 2908.1407\n",
      "Iteration 5200 : Loss 2908.1340\n",
      "Iteration 5210 : Loss 2908.1273\n",
      "Iteration 5220 : Loss 2908.1206\n",
      "Iteration 5230 : Loss 2908.1138\n",
      "Iteration 5240 : Loss 2908.1071\n",
      "Iteration 5250 : Loss 2908.1004\n",
      "Iteration 5260 : Loss 2908.0937\n",
      "Iteration 5270 : Loss 2908.0870\n",
      "Iteration 5280 : Loss 2908.0803\n",
      "Iteration 5290 : Loss 2908.0736\n",
      "Iteration 5300 : Loss 2908.0669\n",
      "Iteration 5310 : Loss 2908.0602\n",
      "Iteration 5320 : Loss 2908.0535\n",
      "Iteration 5330 : Loss 2908.0468\n",
      "Iteration 5340 : Loss 2908.0401\n",
      "Iteration 5350 : Loss 2908.0334\n",
      "Iteration 5360 : Loss 2908.0267\n",
      "Iteration 5370 : Loss 2908.0200\n",
      "Iteration 5380 : Loss 2908.0133\n",
      "Iteration 5390 : Loss 2908.0067\n",
      "Iteration 5400 : Loss 2908.0000\n",
      "Iteration 5410 : Loss 2907.9933\n",
      "Iteration 5420 : Loss 2907.9866\n",
      "Iteration 5430 : Loss 2907.9800\n",
      "Iteration 5440 : Loss 2907.9733\n",
      "Iteration 5450 : Loss 2907.9666\n",
      "Iteration 5460 : Loss 2907.9600\n",
      "Iteration 5470 : Loss 2907.9533\n",
      "Iteration 5480 : Loss 2907.9467\n",
      "Iteration 5490 : Loss 2907.9400\n",
      "Iteration 5500 : Loss 2907.9334\n",
      "Iteration 5510 : Loss 2907.9267\n",
      "Iteration 5520 : Loss 2907.9201\n",
      "Iteration 5530 : Loss 2907.9134\n",
      "Iteration 5540 : Loss 2907.9068\n",
      "Iteration 5550 : Loss 2907.9001\n",
      "Iteration 5560 : Loss 2907.8935\n",
      "Iteration 5570 : Loss 2907.8869\n",
      "Iteration 5580 : Loss 2907.8802\n",
      "Iteration 5590 : Loss 2907.8736\n",
      "Iteration 5600 : Loss 2907.8670\n",
      "Iteration 5610 : Loss 2907.8603\n",
      "Iteration 5620 : Loss 2907.8537\n",
      "Iteration 5630 : Loss 2907.8471\n",
      "Iteration 5640 : Loss 2907.8405\n",
      "Iteration 5650 : Loss 2907.8339\n",
      "Iteration 5660 : Loss 2907.8272\n",
      "Iteration 5670 : Loss 2907.8206\n",
      "Iteration 5680 : Loss 2907.8140\n",
      "Iteration 5690 : Loss 2907.8074\n",
      "Iteration 5700 : Loss 2907.8008\n",
      "Iteration 5710 : Loss 2907.7942\n",
      "Iteration 5720 : Loss 2907.7876\n",
      "Iteration 5730 : Loss 2907.7810\n",
      "Iteration 5740 : Loss 2907.7744\n",
      "Iteration 5750 : Loss 2907.7678\n",
      "Iteration 5760 : Loss 2907.7612\n",
      "Iteration 5770 : Loss 2907.7546\n",
      "Iteration 5780 : Loss 2907.7481\n",
      "Iteration 5790 : Loss 2907.7415\n",
      "Iteration 5800 : Loss 2907.7349\n",
      "Iteration 5810 : Loss 2907.7283\n",
      "Iteration 5820 : Loss 2907.7217\n",
      "Iteration 5830 : Loss 2907.7152\n",
      "Iteration 5840 : Loss 2907.7086\n",
      "Iteration 5850 : Loss 2907.7020\n",
      "Iteration 5860 : Loss 2907.6955\n",
      "Iteration 5870 : Loss 2907.6889\n",
      "Iteration 5880 : Loss 2907.6823\n",
      "Iteration 5890 : Loss 2907.6758\n",
      "Iteration 5900 : Loss 2907.6692\n",
      "Iteration 5910 : Loss 2907.6627\n",
      "Iteration 5920 : Loss 2907.6561\n",
      "Iteration 5930 : Loss 2907.6496\n",
      "Iteration 5940 : Loss 2907.6430\n",
      "Iteration 5950 : Loss 2907.6365\n",
      "Iteration 5960 : Loss 2907.6299\n",
      "Iteration 5970 : Loss 2907.6234\n",
      "Iteration 5980 : Loss 2907.6168\n",
      "Iteration 5990 : Loss 2907.6103\n",
      "Iteration 6000 : Loss 2907.6038\n",
      "Iteration 6010 : Loss 2907.5972\n",
      "Iteration 6020 : Loss 2907.5907\n",
      "Iteration 6030 : Loss 2907.5842\n",
      "Iteration 6040 : Loss 2907.5777\n",
      "Iteration 6050 : Loss 2907.5711\n",
      "Iteration 6060 : Loss 2907.5646\n",
      "Iteration 6070 : Loss 2907.5581\n",
      "Iteration 6080 : Loss 2907.5516\n",
      "Iteration 6090 : Loss 2907.5451\n",
      "Iteration 6100 : Loss 2907.5385\n",
      "Iteration 6110 : Loss 2907.5320\n",
      "Iteration 6120 : Loss 2907.5255\n",
      "Iteration 6130 : Loss 2907.5190\n",
      "Iteration 6140 : Loss 2907.5125\n",
      "Iteration 6150 : Loss 2907.5060\n",
      "Iteration 6160 : Loss 2907.4995\n",
      "Iteration 6170 : Loss 2907.4930\n",
      "Iteration 6180 : Loss 2907.4865\n",
      "Iteration 6190 : Loss 2907.4800\n",
      "Iteration 6200 : Loss 2907.4735\n",
      "Iteration 6210 : Loss 2907.4671\n",
      "Iteration 6220 : Loss 2907.4606\n",
      "Iteration 6230 : Loss 2907.4541\n",
      "Iteration 6240 : Loss 2907.4476\n",
      "Iteration 6250 : Loss 2907.4411\n",
      "Iteration 6260 : Loss 2907.4347\n",
      "Iteration 6270 : Loss 2907.4282\n",
      "Iteration 6280 : Loss 2907.4217\n",
      "Iteration 6290 : Loss 2907.4152\n",
      "Iteration 6300 : Loss 2907.4088\n",
      "Iteration 6310 : Loss 2907.4023\n",
      "Iteration 6320 : Loss 2907.3958\n",
      "Iteration 6330 : Loss 2907.3894\n",
      "Iteration 6340 : Loss 2907.3829\n",
      "Iteration 6350 : Loss 2907.3765\n",
      "Iteration 6360 : Loss 2907.3700\n",
      "Iteration 6370 : Loss 2907.3636\n",
      "Iteration 6380 : Loss 2907.3571\n",
      "Iteration 6390 : Loss 2907.3507\n",
      "Iteration 6400 : Loss 2907.3442\n",
      "Iteration 6410 : Loss 2907.3378\n",
      "Iteration 6420 : Loss 2907.3313\n",
      "Iteration 6430 : Loss 2907.3249\n",
      "Iteration 6440 : Loss 2907.3185\n",
      "Iteration 6450 : Loss 2907.3120\n",
      "Iteration 6460 : Loss 2907.3056\n",
      "Iteration 6470 : Loss 2907.2992\n",
      "Iteration 6480 : Loss 2907.2927\n",
      "Iteration 6490 : Loss 2907.2863\n",
      "Iteration 6500 : Loss 2907.2799\n",
      "Iteration 6510 : Loss 2907.2735\n",
      "Iteration 6520 : Loss 2907.2671\n",
      "Iteration 6530 : Loss 2907.2606\n",
      "Iteration 6540 : Loss 2907.2542\n",
      "Iteration 6550 : Loss 2907.2478\n",
      "Iteration 6560 : Loss 2907.2414\n",
      "Iteration 6570 : Loss 2907.2350\n",
      "Iteration 6580 : Loss 2907.2286\n",
      "Iteration 6590 : Loss 2907.2222\n",
      "Iteration 6600 : Loss 2907.2158\n",
      "Iteration 6610 : Loss 2907.2094\n",
      "Iteration 6620 : Loss 2907.2030\n",
      "Iteration 6630 : Loss 2907.1966\n",
      "Iteration 6640 : Loss 2907.1902\n",
      "Iteration 6650 : Loss 2907.1838\n",
      "Iteration 6660 : Loss 2907.1774\n",
      "Iteration 6670 : Loss 2907.1710\n",
      "Iteration 6680 : Loss 2907.1647\n",
      "Iteration 6690 : Loss 2907.1583\n",
      "Iteration 6700 : Loss 2907.1519\n",
      "Iteration 6710 : Loss 2907.1455\n",
      "Iteration 6720 : Loss 2907.1392\n",
      "Iteration 6730 : Loss 2907.1328\n",
      "Iteration 6740 : Loss 2907.1264\n",
      "Iteration 6750 : Loss 2907.1200\n",
      "Iteration 6760 : Loss 2907.1137\n",
      "Iteration 6770 : Loss 2907.1073\n",
      "Iteration 6780 : Loss 2907.1009\n",
      "Iteration 6790 : Loss 2907.0946\n",
      "Iteration 6800 : Loss 2907.0882\n",
      "Iteration 6810 : Loss 2907.0819\n",
      "Iteration 6820 : Loss 2907.0755\n",
      "Iteration 6830 : Loss 2907.0692\n",
      "Iteration 6840 : Loss 2907.0628\n",
      "Iteration 6850 : Loss 2907.0565\n",
      "Iteration 6860 : Loss 2907.0501\n",
      "Iteration 6870 : Loss 2907.0438\n",
      "Iteration 6880 : Loss 2907.0375\n",
      "Iteration 6890 : Loss 2907.0311\n",
      "Iteration 6900 : Loss 2907.0248\n",
      "Iteration 6910 : Loss 2907.0184\n",
      "Iteration 6920 : Loss 2907.0121\n",
      "Iteration 6930 : Loss 2907.0058\n",
      "Iteration 6940 : Loss 2906.9995\n",
      "Iteration 6950 : Loss 2906.9931\n",
      "Iteration 6960 : Loss 2906.9868\n",
      "Iteration 6970 : Loss 2906.9805\n",
      "Iteration 6980 : Loss 2906.9742\n",
      "Iteration 6990 : Loss 2906.9679\n",
      "Iteration 7000 : Loss 2906.9615\n",
      "Iteration 7010 : Loss 2906.9552\n",
      "Iteration 7020 : Loss 2906.9489\n",
      "Iteration 7030 : Loss 2906.9426\n",
      "Iteration 7040 : Loss 2906.9363\n",
      "Iteration 7050 : Loss 2906.9300\n",
      "Iteration 7060 : Loss 2906.9237\n",
      "Iteration 7070 : Loss 2906.9174\n",
      "Iteration 7080 : Loss 2906.9111\n",
      "Iteration 7090 : Loss 2906.9048\n",
      "Iteration 7100 : Loss 2906.8985\n",
      "Iteration 7110 : Loss 2906.8922\n",
      "Iteration 7120 : Loss 2906.8859\n",
      "Iteration 7130 : Loss 2906.8797\n",
      "Iteration 7140 : Loss 2906.8734\n",
      "Iteration 7150 : Loss 2906.8671\n",
      "Iteration 7160 : Loss 2906.8608\n",
      "Iteration 7170 : Loss 2906.8545\n",
      "Iteration 7180 : Loss 2906.8483\n",
      "Iteration 7190 : Loss 2906.8420\n",
      "Iteration 7200 : Loss 2906.8357\n",
      "Iteration 7210 : Loss 2906.8294\n",
      "Iteration 7220 : Loss 2906.8232\n",
      "Iteration 7230 : Loss 2906.8169\n",
      "Iteration 7240 : Loss 2906.8106\n",
      "Iteration 7250 : Loss 2906.8044\n",
      "Iteration 7260 : Loss 2906.7981\n",
      "Iteration 7270 : Loss 2906.7919\n",
      "Iteration 7280 : Loss 2906.7856\n",
      "Iteration 7290 : Loss 2906.7794\n",
      "Iteration 7300 : Loss 2906.7731\n",
      "Iteration 7310 : Loss 2906.7669\n",
      "Iteration 7320 : Loss 2906.7606\n",
      "Iteration 7330 : Loss 2906.7544\n",
      "Iteration 7340 : Loss 2906.7481\n",
      "Iteration 7350 : Loss 2906.7419\n",
      "Iteration 7360 : Loss 2906.7357\n",
      "Iteration 7370 : Loss 2906.7294\n",
      "Iteration 7380 : Loss 2906.7232\n",
      "Iteration 7390 : Loss 2906.7170\n",
      "Iteration 7400 : Loss 2906.7107\n",
      "Iteration 7410 : Loss 2906.7045\n",
      "Iteration 7420 : Loss 2906.6983\n",
      "Iteration 7430 : Loss 2906.6921\n",
      "Iteration 7440 : Loss 2906.6858\n",
      "Iteration 7450 : Loss 2906.6796\n",
      "Iteration 7460 : Loss 2906.6734\n",
      "Iteration 7470 : Loss 2906.6672\n",
      "Iteration 7480 : Loss 2906.6610\n",
      "Iteration 7490 : Loss 2906.6548\n",
      "Iteration 7500 : Loss 2906.6486\n",
      "Iteration 7510 : Loss 2906.6423\n",
      "Iteration 7520 : Loss 2906.6361\n",
      "Iteration 7530 : Loss 2906.6299\n",
      "Iteration 7540 : Loss 2906.6237\n",
      "Iteration 7550 : Loss 2906.6175\n",
      "Iteration 7560 : Loss 2906.6114\n",
      "Iteration 7570 : Loss 2906.6052\n",
      "Iteration 7580 : Loss 2906.5990\n",
      "Iteration 7590 : Loss 2906.5928\n",
      "Iteration 7600 : Loss 2906.5866\n",
      "Iteration 7610 : Loss 2906.5804\n",
      "Iteration 7620 : Loss 2906.5742\n",
      "Iteration 7630 : Loss 2906.5680\n",
      "Iteration 7640 : Loss 2906.5619\n",
      "Iteration 7650 : Loss 2906.5557\n",
      "Iteration 7660 : Loss 2906.5495\n",
      "Iteration 7670 : Loss 2906.5433\n",
      "Iteration 7680 : Loss 2906.5372\n",
      "Iteration 7690 : Loss 2906.5310\n",
      "Iteration 7700 : Loss 2906.5248\n",
      "Iteration 7710 : Loss 2906.5187\n",
      "Iteration 7720 : Loss 2906.5125\n",
      "Iteration 7730 : Loss 2906.5064\n",
      "Iteration 7740 : Loss 2906.5002\n",
      "Iteration 7750 : Loss 2906.4940\n",
      "Iteration 7760 : Loss 2906.4879\n",
      "Iteration 7770 : Loss 2906.4817\n",
      "Iteration 7780 : Loss 2906.4756\n",
      "Iteration 7790 : Loss 2906.4694\n",
      "Iteration 7800 : Loss 2906.4633\n",
      "Iteration 7810 : Loss 2906.4572\n",
      "Iteration 7820 : Loss 2906.4510\n",
      "Iteration 7830 : Loss 2906.4449\n",
      "Iteration 7840 : Loss 2906.4387\n",
      "Iteration 7850 : Loss 2906.4326\n",
      "Iteration 7860 : Loss 2906.4265\n",
      "Iteration 7870 : Loss 2906.4203\n",
      "Iteration 7880 : Loss 2906.4142\n",
      "Iteration 7890 : Loss 2906.4081\n",
      "Iteration 7900 : Loss 2906.4020\n",
      "Iteration 7910 : Loss 2906.3958\n",
      "Iteration 7920 : Loss 2906.3897\n",
      "Iteration 7930 : Loss 2906.3836\n",
      "Iteration 7940 : Loss 2906.3775\n",
      "Iteration 7950 : Loss 2906.3714\n",
      "Iteration 7960 : Loss 2906.3653\n",
      "Iteration 7970 : Loss 2906.3592\n",
      "Iteration 7980 : Loss 2906.3530\n",
      "Iteration 7990 : Loss 2906.3469\n",
      "Iteration 8000 : Loss 2906.3408\n",
      "Iteration 8010 : Loss 2906.3347\n",
      "Iteration 8020 : Loss 2906.3286\n",
      "Iteration 8030 : Loss 2906.3225\n",
      "Iteration 8040 : Loss 2906.3164\n",
      "Iteration 8050 : Loss 2906.3103\n",
      "Iteration 8060 : Loss 2906.3043\n",
      "Iteration 8070 : Loss 2906.2982\n",
      "Iteration 8080 : Loss 2906.2921\n",
      "Iteration 8090 : Loss 2906.2860\n",
      "Iteration 8100 : Loss 2906.2799\n",
      "Iteration 8110 : Loss 2906.2738\n",
      "Iteration 8120 : Loss 2906.2678\n",
      "Iteration 8130 : Loss 2906.2617\n",
      "Iteration 8140 : Loss 2906.2556\n",
      "Iteration 8150 : Loss 2906.2495\n",
      "Iteration 8160 : Loss 2906.2435\n",
      "Iteration 8170 : Loss 2906.2374\n",
      "Iteration 8180 : Loss 2906.2313\n",
      "Iteration 8190 : Loss 2906.2253\n",
      "Iteration 8200 : Loss 2906.2192\n",
      "Iteration 8210 : Loss 2906.2131\n",
      "Iteration 8220 : Loss 2906.2071\n",
      "Iteration 8230 : Loss 2906.2010\n",
      "Iteration 8240 : Loss 2906.1950\n",
      "Iteration 8250 : Loss 2906.1889\n",
      "Iteration 8260 : Loss 2906.1829\n",
      "Iteration 8270 : Loss 2906.1768\n",
      "Iteration 8280 : Loss 2906.1708\n",
      "Iteration 8290 : Loss 2906.1647\n",
      "Iteration 8300 : Loss 2906.1587\n",
      "Iteration 8310 : Loss 2906.1527\n",
      "Iteration 8320 : Loss 2906.1466\n",
      "Iteration 8330 : Loss 2906.1406\n",
      "Iteration 8340 : Loss 2906.1345\n",
      "Iteration 8350 : Loss 2906.1285\n",
      "Iteration 8360 : Loss 2906.1225\n",
      "Iteration 8370 : Loss 2906.1165\n",
      "Iteration 8380 : Loss 2906.1104\n",
      "Iteration 8390 : Loss 2906.1044\n",
      "Iteration 8400 : Loss 2906.0984\n",
      "Iteration 8410 : Loss 2906.0924\n",
      "Iteration 8420 : Loss 2906.0864\n",
      "Iteration 8430 : Loss 2906.0803\n",
      "Iteration 8440 : Loss 2906.0743\n",
      "Iteration 8450 : Loss 2906.0683\n",
      "Iteration 8460 : Loss 2906.0623\n",
      "Iteration 8470 : Loss 2906.0563\n",
      "Iteration 8480 : Loss 2906.0503\n",
      "Iteration 8490 : Loss 2906.0443\n",
      "Iteration 8500 : Loss 2906.0383\n",
      "Iteration 8510 : Loss 2906.0323\n",
      "Iteration 8520 : Loss 2906.0263\n",
      "Iteration 8530 : Loss 2906.0203\n",
      "Iteration 8540 : Loss 2906.0143\n",
      "Iteration 8550 : Loss 2906.0083\n",
      "Iteration 8560 : Loss 2906.0023\n",
      "Iteration 8570 : Loss 2905.9963\n",
      "Iteration 8580 : Loss 2905.9904\n",
      "Iteration 8590 : Loss 2905.9844\n",
      "Iteration 8600 : Loss 2905.9784\n",
      "Iteration 8610 : Loss 2905.9724\n",
      "Iteration 8620 : Loss 2905.9664\n",
      "Iteration 8630 : Loss 2905.9605\n",
      "Iteration 8640 : Loss 2905.9545\n",
      "Iteration 8650 : Loss 2905.9485\n",
      "Iteration 8660 : Loss 2905.9425\n",
      "Iteration 8670 : Loss 2905.9366\n",
      "Iteration 8680 : Loss 2905.9306\n",
      "Iteration 8690 : Loss 2905.9247\n",
      "Iteration 8700 : Loss 2905.9187\n",
      "Iteration 8710 : Loss 2905.9127\n",
      "Iteration 8720 : Loss 2905.9068\n",
      "Iteration 8730 : Loss 2905.9008\n",
      "Iteration 8740 : Loss 2905.8949\n",
      "Iteration 8750 : Loss 2905.8889\n",
      "Iteration 8760 : Loss 2905.8830\n",
      "Iteration 8770 : Loss 2905.8770\n",
      "Iteration 8780 : Loss 2905.8711\n",
      "Iteration 8790 : Loss 2905.8651\n",
      "Iteration 8800 : Loss 2905.8592\n",
      "Iteration 8810 : Loss 2905.8533\n",
      "Iteration 8820 : Loss 2905.8473\n",
      "Iteration 8830 : Loss 2905.8414\n",
      "Iteration 8840 : Loss 2905.8355\n",
      "Iteration 8850 : Loss 2905.8295\n",
      "Iteration 8860 : Loss 2905.8236\n",
      "Iteration 8870 : Loss 2905.8177\n",
      "Iteration 8880 : Loss 2905.8118\n",
      "Iteration 8890 : Loss 2905.8058\n",
      "Iteration 8900 : Loss 2905.7999\n",
      "Iteration 8910 : Loss 2905.7940\n",
      "Iteration 8920 : Loss 2905.7881\n",
      "Iteration 8930 : Loss 2905.7822\n",
      "Iteration 8940 : Loss 2905.7763\n",
      "Iteration 8950 : Loss 2905.7703\n",
      "Iteration 8960 : Loss 2905.7644\n",
      "Iteration 8970 : Loss 2905.7585\n",
      "Iteration 8980 : Loss 2905.7526\n",
      "Iteration 8990 : Loss 2905.7467\n",
      "Iteration 9000 : Loss 2905.7408\n",
      "Iteration 9010 : Loss 2905.7349\n",
      "Iteration 9020 : Loss 2905.7290\n",
      "Iteration 9030 : Loss 2905.7231\n",
      "Iteration 9040 : Loss 2905.7172\n",
      "Iteration 9050 : Loss 2905.7114\n",
      "Iteration 9060 : Loss 2905.7055\n",
      "Iteration 9070 : Loss 2905.6996\n",
      "Iteration 9080 : Loss 2905.6937\n",
      "Iteration 9090 : Loss 2905.6878\n",
      "Iteration 9100 : Loss 2905.6819\n",
      "Iteration 9110 : Loss 2905.6761\n",
      "Iteration 9120 : Loss 2905.6702\n",
      "Iteration 9130 : Loss 2905.6643\n",
      "Iteration 9140 : Loss 2905.6584\n",
      "Iteration 9150 : Loss 2905.6526\n",
      "Iteration 9160 : Loss 2905.6467\n",
      "Iteration 9170 : Loss 2905.6408\n",
      "Iteration 9180 : Loss 2905.6350\n",
      "Iteration 9190 : Loss 2905.6291\n",
      "Iteration 9200 : Loss 2905.6232\n",
      "Iteration 9210 : Loss 2905.6174\n",
      "Iteration 9220 : Loss 2905.6115\n",
      "Iteration 9230 : Loss 2905.6057\n",
      "Iteration 9240 : Loss 2905.5998\n",
      "Iteration 9250 : Loss 2905.5940\n",
      "Iteration 9260 : Loss 2905.5881\n",
      "Iteration 9270 : Loss 2905.5823\n",
      "Iteration 9280 : Loss 2905.5764\n",
      "Iteration 9290 : Loss 2905.5706\n",
      "Iteration 9300 : Loss 2905.5647\n",
      "Iteration 9310 : Loss 2905.5589\n",
      "Iteration 9320 : Loss 2905.5531\n",
      "Iteration 9330 : Loss 2905.5472\n",
      "Iteration 9340 : Loss 2905.5414\n",
      "Iteration 9350 : Loss 2905.5356\n",
      "Iteration 9360 : Loss 2905.5297\n",
      "Iteration 9370 : Loss 2905.5239\n",
      "Iteration 9380 : Loss 2905.5181\n",
      "Iteration 9390 : Loss 2905.5123\n",
      "Iteration 9400 : Loss 2905.5065\n",
      "Iteration 9410 : Loss 2905.5006\n",
      "Iteration 9420 : Loss 2905.4948\n",
      "Iteration 9430 : Loss 2905.4890\n",
      "Iteration 9440 : Loss 2905.4832\n",
      "Iteration 9450 : Loss 2905.4774\n",
      "Iteration 9460 : Loss 2905.4716\n",
      "Iteration 9470 : Loss 2905.4658\n",
      "Iteration 9480 : Loss 2905.4600\n",
      "Iteration 9490 : Loss 2905.4542\n",
      "Iteration 9500 : Loss 2905.4484\n",
      "Iteration 9510 : Loss 2905.4426\n",
      "Iteration 9520 : Loss 2905.4368\n",
      "Iteration 9530 : Loss 2905.4310\n",
      "Iteration 9540 : Loss 2905.4252\n",
      "Iteration 9550 : Loss 2905.4194\n",
      "Iteration 9560 : Loss 2905.4136\n",
      "Iteration 9570 : Loss 2905.4078\n",
      "Iteration 9580 : Loss 2905.4020\n",
      "Iteration 9590 : Loss 2905.3962\n",
      "Iteration 9600 : Loss 2905.3905\n",
      "Iteration 9610 : Loss 2905.3847\n",
      "Iteration 9620 : Loss 2905.3789\n",
      "Iteration 9630 : Loss 2905.3731\n",
      "Iteration 9640 : Loss 2905.3673\n",
      "Iteration 9650 : Loss 2905.3616\n",
      "Iteration 9660 : Loss 2905.3558\n",
      "Iteration 9670 : Loss 2905.3500\n",
      "Iteration 9680 : Loss 2905.3443\n",
      "Iteration 9690 : Loss 2905.3385\n",
      "Iteration 9700 : Loss 2905.3327\n",
      "Iteration 9710 : Loss 2905.3270\n",
      "Iteration 9720 : Loss 2905.3212\n",
      "Iteration 9730 : Loss 2905.3155\n",
      "Iteration 9740 : Loss 2905.3097\n",
      "Iteration 9750 : Loss 2905.3040\n",
      "Iteration 9760 : Loss 2905.2982\n",
      "Iteration 9770 : Loss 2905.2925\n",
      "Iteration 9780 : Loss 2905.2867\n",
      "Iteration 9790 : Loss 2905.2810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9800 : Loss 2905.2752\n",
      "Iteration 9810 : Loss 2905.2695\n",
      "Iteration 9820 : Loss 2905.2638\n",
      "Iteration 9830 : Loss 2905.2580\n",
      "Iteration 9840 : Loss 2905.2523\n",
      "Iteration 9850 : Loss 2905.2466\n",
      "Iteration 9860 : Loss 2905.2408\n",
      "Iteration 9870 : Loss 2905.2351\n",
      "Iteration 9880 : Loss 2905.2294\n",
      "Iteration 9890 : Loss 2905.2236\n",
      "Iteration 9900 : Loss 2905.2179\n",
      "Iteration 9910 : Loss 2905.2122\n",
      "Iteration 9920 : Loss 2905.2065\n",
      "Iteration 9930 : Loss 2905.2008\n",
      "Iteration 9940 : Loss 2905.1951\n",
      "Iteration 9950 : Loss 2905.1893\n",
      "Iteration 9960 : Loss 2905.1836\n",
      "Iteration 9970 : Loss 2905.1779\n",
      "Iteration 9980 : Loss 2905.1722\n",
      "Iteration 9990 : Loss 2905.1665\n",
      "Iteration 10000 : Loss 2905.1608\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(10001):\n",
    "    y_pred = model(X_train, W, b)\n",
    "    L = MSE(y_pred, y_train)\n",
    "    dW, db = gradient(X_train, y_train, y_pred)\n",
    "    \n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    \n",
    "    losses.append(L)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration {i} : Loss {L:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "vzI0vVVSLDeg",
    "outputId": "8758e1fe-caec-4e8e-eb74-b19f5cef4e26"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnbommRsKQtw"
   },
   "source": [
    "## test 데이터에 대한 성능 확인하기\n",
    "test 데이터에 대한 성능을 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUd7uuqHTfmY",
    "outputId": "5a30ed3b-46f5-4c8e-b720-46739e83829f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881.820025641594"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFfKF-vyKScV"
   },
   "source": [
    "## 정답 데이터와 예측한 데이터 시각화하기\n",
    "x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "J5D82Eo-LEOa",
    "outputId": "9b4b2317-8830-4f43-c516-53f7c45d3f9a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], prediction, label='pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회고\n",
    "맨바닥에 코드를 쓰라고 하면 아직은 쓸 수 없는 단계라, 학습 노드와 챗GPT 그리고 함께 진행한 팀원들의 코드를 많이 참조했다.\n",
    "어제 할 때에는 정말 정신이 없었는데, 오늘 다시 차분히 정리해보니 내용들이 조금씩 눈에 들어온다.\n",
    "\n",
    "챗GPT로 코드를 다듬어달라고 요청하는 것으로 마무리했는데, 중복을 제거하고 단순화하는 변경지점을 보면서 많은 학습이 되었다.\n",
    "예를 들어서 기울기 벡터 모델을 만드는 원래 코드는 아래와 같았는데,\n",
    "\n",
    "```\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(X_train.shape[1]):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions\n",
    "```\n",
    "수정 후에는 아래와 같이 바뀌었다.\n",
    "```\n",
    "def model(X, W, b):\n",
    "    return X.dot(W) + b\n",
    "```\n",
    "이 코드는 함께 공부한 팀원인 서연님의 코드이기도 하다. 수리 연산 메서드를 바로 사용하면 코드가 훨씬 간결해지는 것 같다.\n",
    "물론 내적 개념도, 메서드도 잘 모르는 상황이었기 때문에 추가적인 학습이 필요했다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
